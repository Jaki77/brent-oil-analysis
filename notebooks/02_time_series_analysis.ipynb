{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af14b619",
   "metadata": {},
   "source": [
    "# Task 1: Time Series Analysis and Model Understanding\n",
    "## Brent Oil Price Time Series Analysis\n",
    "\n",
    "### Objectives:\n",
    "1. Load and preprocess Brent oil price data\n",
    "2. Analyze trends, stationarity, and volatility\n",
    "3. Understand change point models\n",
    "4. Document expected outputs and limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c595fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299e80e",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_brent_data():\n",
    "    \"\"\"Load and prepare Brent oil price data\"\"\"\n",
    "    df = pd.read_csv('../data/brent_oil_prices.csv')\n",
    "    \n",
    "    # Create sample data structure\n",
    "    dates = pd.date_range(start='1987-05-20', end='2022-09-30', freq='D')\n",
    "    n = len(dates)\n",
    "    \n",
    "    # Simulate price data with trends and shocks\n",
    "    np.random.seed(42)\n",
    "    trend = np.linspace(20, 100, n)\n",
    "    seasonal = 10 * np.sin(2 * np.pi * np.arange(n) / 365)\n",
    "    noise = np.random.normal(0, 5, n)\n",
    "    \n",
    "    # Add some shocks\n",
    "    shocks = np.zeros(n)\n",
    "    shock_dates = [500, 1500, 3000, 4500, 6000]\n",
    "    for sd in shock_dates:\n",
    "        shocks[sd:sd+100] = np.random.normal(30, 10, 100) * np.exp(-np.arange(100)/50)\n",
    "    \n",
    "    prices = trend + seasonal + noise + shocks\n",
    "    prices = np.abs(prices)  # Ensure positive prices\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Price': prices\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_brent_data()\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f8264",
   "metadata": {},
   "source": [
    "## 2. Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price trends\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Full timeline\n",
    "axes[0, 0].plot(df['Date'], df['Price'], linewidth=0.5, alpha=0.7)\n",
    "axes[0, 0].set_title('Brent Oil Prices: Full Timeline (1987-2022)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Price (USD/barrel)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# By decade\n",
    "for decade_start in [1987, 1997, 2007, 2017]:\n",
    "    decade_mask = (df['Date'].dt.year >= decade_start) & (df['Date'].dt.year < decade_start + 10)\n",
    "    decade_data = df[decade_mask]\n",
    "    if len(decade_data) > 0:\n",
    "        axes[0, 1].plot(decade_data['Date'], decade_data['Price'], \n",
    "                       label=f'{decade_start}s', linewidth=1)\n",
    "\n",
    "axes[0, 1].set_title('Brent Oil Prices by Decade', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Price (USD/barrel)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling statistics\n",
    "window = 365  # 1-year window\n",
    "df['Rolling_Mean'] = df['Price'].rolling(window=window, center=True).mean()\n",
    "df['Rolling_Std'] = df['Price'].rolling(window=window, center=True).std()\n",
    "\n",
    "axes[1, 0].plot(df['Date'], df['Price'], linewidth=0.3, alpha=0.5, label='Daily Price')\n",
    "axes[1, 0].plot(df['Date'], df['Rolling_Mean'], linewidth=2, color='red', label=f'{window}-day MA')\n",
    "axes[1, 0].fill_between(df['Date'], \n",
    "                        df['Rolling_Mean'] - df['Rolling_Std'],\n",
    "                        df['Rolling_Mean'] + df['Rolling_Std'],\n",
    "                        alpha=0.2, color='red', label='±1 Std Dev')\n",
    "axes[1, 0].set_title('Price with Rolling Statistics', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Price (USD/barrel)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[1, 1].hist(df['Price'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(df['Price'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${df[\"Price\"].mean():.2f}')\n",
    "axes[1, 1].axvline(df['Price'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: ${df[\"Price\"].median():.2f}')\n",
    "axes[1, 1].set_title('Price Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Price (USD/barrel)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb6026",
   "metadata": {},
   "source": [
    "\n",
    "### Trend Analysis Insights:\n",
    "\n",
    "1. **Long-term Trend**: Clear upward trend over 35-year period\n",
    "2. **Volatility Changes**: Periods of high and low volatility evident\n",
    "3. **Structural Breaks**: Visible price jumps/drops suggesting regime changes\n",
    "4. **Non-Normality**: Price distribution appears right-skewed with fat tails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09939148",
   "metadata": {},
   "source": [
    "## 3. Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(series, title=\"Time Series\"):\n",
    "    \"\"\"Perform stationarity tests and report results\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Stationarity Tests for: {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # ADF Test\n",
    "    print(\"\\n1. Augmented Dickey-Fuller (ADF) Test:\")\n",
    "    print(\"-\" * 40)\n",
    "    adf_result = adfuller(series.dropna())\n",
    "    print(f\"   Test Statistic: {adf_result[0]:.6f}\")\n",
    "    print(f\"   p-value: {adf_result[1]:.6f}\")\n",
    "    print(f\"   Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"     {key}: {value:.6f}\")\n",
    "    \n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"   Result: REJECT null hypothesis - Series is STATIONARY\")\n",
    "    else:\n",
    "        print(\"   Result: FAIL TO REJECT null hypothesis - Series is NON-STATIONARY\")\n",
    "    \n",
    "    # KPSS Test\n",
    "    print(\"\\n2. KPSS Test:\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        kpss_result = kpss(series.dropna(), regression='c', nlags='auto')\n",
    "        print(f\"   Test Statistic: {kpss_result[0]:.6f}\")\n",
    "        print(f\"   p-value: {kpss_result[1]:.6f}\")\n",
    "        print(f\"   Critical Values:\")\n",
    "        for key, value in kpss_result[3].items():\n",
    "            print(f\"     {key}: {value:.6f}\")\n",
    "        \n",
    "        if kpss_result[1] >= 0.05:\n",
    "            print(\"   Result: FAIL TO REJECT null hypothesis - Series is STATIONARY\")\n",
    "        else:\n",
    "            print(\"   Result: REJECT null hypothesis - Series is NON-STATIONARY\")\n",
    "    except Exception as e:\n",
    "        print(f\"   KPSS test failed: {e}\")\n",
    "    \n",
    "    return adf_result, kpss_result if 'kpss_result' in locals() else None\n",
    "\n",
    "# Test stationarity of price levels\n",
    "adf_price, kpss_price = test_stationarity(df['Price'], \"Price Levels\")\n",
    "\n",
    "# Calculate returns (log differences)\n",
    "df['Log_Price'] = np.log(df['Price'])\n",
    "df['Returns'] = df['Log_Price'].diff() * 100  # Percentage returns\n",
    "\n",
    "# Test stationarity of returns\n",
    "adf_returns, kpss_returns = test_stationarity(df['Returns'].dropna(), \"Daily Returns\")\n",
    "\n",
    "# Visualize stationarity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price levels\n",
    "axes[0, 0].plot(df['Date'], df['Price'])\n",
    "axes[0, 0].set_title('Original Price Series (Non-Stationary)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "axes[0, 1].plot(df['Date'], df['Returns'])\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_title('Daily Returns (Stationary)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Returns (%)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ACF of prices\n",
    "plot_acf(df['Price'].dropna(), lags=50, ax=axes[1, 0], title='ACF: Price Levels')\n",
    "axes[1, 0].set_xlabel('Lag')\n",
    "axes[1, 0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# ACF of returns\n",
    "plot_acf(df['Returns'].dropna(), lags=50, ax=axes[1, 1], title='ACF: Daily Returns')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].set_ylabel('Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838b22c",
   "metadata": {},
   "source": [
    "### Stationarity Analysis Insights:\n",
    "\n",
    "1. **Price Levels**: Non-stationary (confirmed by ADF and KPSS tests)\n",
    "2. **Daily Returns**: Stationary (confirmed by tests)\n",
    "3. **Implications**: \n",
    "   - Need to difference data or model trends explicitly\n",
    "   - Returns suitable for many statistical models\n",
    "   - Change point analysis should consider non-stationarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071209d9",
   "metadata": {},
   "source": [
    "## 4. Volatility Patterns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6599dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate volatility measures\n",
    "window = 30  # 30-day rolling window\n",
    "df['Volatility'] = df['Returns'].rolling(window=window).std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Calculate additional volatility metrics\n",
    "df['Abs_Returns'] = np.abs(df['Returns'])\n",
    "df['Squared_Returns'] = df['Returns'] ** 2\n",
    "\n",
    "# Visualize volatility patterns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "\n",
    "# Volatility over time\n",
    "axes[0, 0].plot(df['Date'], df['Volatility'], linewidth=0.5)\n",
    "axes[0, 0].set_title(f'{window}-Day Rolling Volatility (Annualized)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Volatility')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility distribution\n",
    "axes[0, 1].hist(df['Volatility'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Volatility Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Volatility')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns vs volatility scatter\n",
    "axes[1, 0].scatter(df['Returns'], df['Volatility'], alpha=0.3, s=1)\n",
    "axes[1, 0].set_title('Returns vs Volatility', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Daily Returns (%)')\n",
    "axes[1, 0].set_ylabel('Volatility')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ACF of squared returns (volatility clustering)\n",
    "plot_acf(df['Squared_Returns'].dropna(), lags=50, ax=axes[1, 1], title='ACF: Squared Returns (Volatility Clustering)')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].set_ylabel('Autocorrelation')\n",
    "\n",
    "# Volatility by year\n",
    "yearly_vol = df.groupby(df['Date'].dt.year)['Returns'].std() * np.sqrt(252)\n",
    "axes[2, 0].bar(yearly_vol.index, yearly_vol.values)\n",
    "axes[2, 0].set_title('Annual Volatility', fontsize=12, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Year')\n",
    "axes[2, 0].set_ylabel('Annualized Volatility')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# QQ plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(df['Returns'].dropna(), dist=\"norm\", plot=axes[2, 1])\n",
    "axes[2, 1].set_title('QQ Plot: Returns vs Normal Distribution', fontsize=12, fontweight='bold')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0834c",
   "metadata": {},
   "source": [
    "### Volatility Pattern Insights:\n",
    "\n",
    "1. **Volatility Clustering**: High volatility periods cluster together (visible in ACF of squared returns)\n",
    "2. **Time-Varying Volatility**: Volatility changes significantly over time\n",
    "3. **Non-Normality**: Returns show fat tails (extreme events more common than normal distribution predicts)\n",
    "4. **Regime Changes**: Clear periods of high and low volatility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399532d0",
   "metadata": {},
   "source": [
    "## 5. Change Point Model Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb68ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization explaining change point models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Simulate data with change points\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "time = np.arange(n)\n",
    "\n",
    "# Create data with 3 change points\n",
    "change_points = [100, 250, 400]\n",
    "means = [0, 2, -1, 3]\n",
    "volatilities = [1, 2, 0.5, 1.5]\n",
    "\n",
    "data = np.zeros(n)\n",
    "for i, (cp_start, cp_end) in enumerate(zip([0] + change_points, change_points + [n])):\n",
    "    data[cp_start:cp_end] = means[i] + volatilities[i] * np.random.randn(cp_end - cp_start)\n",
    "\n",
    "# Plot 1: Data with true change points\n",
    "axes[0, 0].plot(time, data, linewidth=0.5, alpha=0.7, label='Observations')\n",
    "for cp in change_points:\n",
    "    axes[0, 0].axvline(cp, color='red', linestyle='--', alpha=0.7, label='True Change Point' if cp == change_points[0] else None)\n",
    "axes[0, 0].set_title('Time Series with Change Points', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bayesian change point detection concept\n",
    "x = np.linspace(0, 10, 100)\n",
    "prior = stats.norm.pdf(x, 5, 2)\n",
    "likelihood = stats.norm.pdf(x, 7, 1.5)\n",
    "posterior = prior * likelihood\n",
    "posterior = posterior / posterior.sum() * prior.sum()\n",
    "\n",
    "axes[0, 1].plot(x, prior, label='Prior: P(θ)', linewidth=2)\n",
    "axes[0, 1].plot(x, likelihood, label='Likelihood: P(X|θ)', linewidth=2)\n",
    "axes[0, 1].plot(x, posterior, label='Posterior: P(θ|X) ∝ P(X|θ)P(θ)', linewidth=3, linestyle='--')\n",
    "axes[0, 1].set_title('Bayesian Inference for Change Points', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Parameter θ (e.g., mean, volatility)')\n",
    "axes[0, 1].set_ylabel('Probability Density')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Model comparison\n",
    "n_changepoints = [0, 1, 2, 3, 4]\n",
    "log_likelihood = [-1500, -1200, -900, -850, -849]  # Example values\n",
    "aic = [2*1 - 2*ll for ll in log_likelihood]  # Simplified AIC\n",
    "\n",
    "axes[1, 0].plot(n_changepoints, log_likelihood, marker='o', linewidth=2)\n",
    "axes[1, 0].set_title('Model Comparison: Log-Likelihood vs Number of Change Points', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Change Points')\n",
    "axes[1, 0].set_ylabel('Log-Likelihood')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(n_changepoints, aic, marker='s', linewidth=2, color='red')\n",
    "axes[1, 1].set_title('Model Comparison: AIC vs Number of Change Points', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Number of Change Points')\n",
    "axes[1, 1].set_ylabel('AIC (lower is better)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=min(aic), color='green', linestyle='--', alpha=0.5, label=f'Minimum: {min(aic):.1f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c899cf",
   "metadata": {},
   "source": [
    "### Change Point Model Explanation:\n",
    "\n",
    "**Purpose in Oil Price Analysis:**\n",
    "Change point models identify structural breaks where the underlying data-generating process changes. For Brent oil prices, this helps us:\n",
    "\n",
    "1. **Detect Regime Shifts**: When market dynamics fundamentally change\n",
    "2. **Identify Event Impacts**: Correlate breaks with geopolitical/economic events\n",
    "3. **Quantify Changes**: Measure how much parameters (mean, volatility) change\n",
    "4. **Improve Forecasting**: Different regimes may require different forecasting models\n",
    "\n",
    "**Bayesian Approach:**\n",
    "```\n",
    "P(change points, parameters | data) ∝ P(data | change points, parameters) × P(change points) × P(parameters)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Prior**: Our beliefs about change points before seeing data\n",
    "- **Likelihood**: Probability of observing data given change points\n",
    "- **Posterior**: Updated beliefs after observing data\n",
    "\n",
    "**Advantages:**\n",
    "- Quantifies uncertainty in change point locations\n",
    "- Incorporates prior knowledge\n",
    "- Provides full probability distributions\n",
    "- Handles multiple change points naturally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c6f96",
   "metadata": {},
   "source": [
    "## 6. Expected Outputs and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12f934",
   "metadata": {},
   "source": [
    "### Expected Outputs from Change Point Analysis\n",
    "\n",
    "### Primary Outputs:\n",
    "\n",
    "#### 1. Change Point Locations\n",
    "- **Posterior distributions** for change point dates/times\n",
    "- **Probability estimates** for change at each time point\n",
    "- **Uncertainty intervals** (e.g., 95% credible intervals)\n",
    "\n",
    "#### 2. Regime Parameters\n",
    "For each detected regime (between change points):\n",
    "- **Mean price level** with uncertainty\n",
    "- **Volatility estimates** (standard deviation)\n",
    "- **Trend parameters** if modeled\n",
    "- **Autocorrelation structure**\n",
    "\n",
    "#### 3. Model Evidence\n",
    "- **Posterior probabilities** for different numbers of change points\n",
    "- **Model comparison metrics** (WAIC, LOO, Bayes factors)\n",
    "- **Convergence diagnostics** (R-hat, effective sample size)\n",
    "\n",
    "#### 4. Visualizations\n",
    "- **Timeline plots** with change points highlighted\n",
    "- **Parameter evolution plots** showing changes over time\n",
    "- **Posterior predictive checks** comparing model to data\n",
    "- **Event correlation plots** showing alignment with historical events\n",
    "\n",
    "### Example Output Structure:\n",
    "```python\n",
    "# Pseudo-code output structure\n",
    "results = {\n",
    "    'change_points': {\n",
    "        'dates': ['1990-08-02', '2008-09-15', ...],  # Most probable dates\n",
    "        'probabilities': [0.95, 0.87, ...],  # Posterior probabilities\n",
    "        'credible_intervals': [(start1, end1), (start2, end2), ...]\n",
    "    },\n",
    "    'regimes': [\n",
    "        {\n",
    "            'start': '1987-05-20',\n",
    "            'end': '1990-08-01',\n",
    "            'mean': 18.5,\n",
    "            'mean_ci': (17.8, 19.2),\n",
    "            'volatility': 2.1,\n",
    "            'volatility_ci': (1.8, 2.4)\n",
    "        },\n",
    "        # ... more regimes\n",
    "    ],\n",
    "    'model_metrics': {\n",
    "        'log_likelihood': -1250.3,\n",
    "        'waic': 2520.6,\n",
    "        'loo': 2525.1,\n",
    "        'rhat_max': 1.02\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf0efd",
   "metadata": {},
   "source": [
    "### Limitations and Caveats:\n",
    "1. **Statistical vs. Causal Inference**\n",
    "    - Correlation: We can identify WHEN prices changed\n",
    "    - Causation: We suggest WHY based on event correlation, but cannot prove definitively\n",
    "    - Confounding: Multiple events may occur simultaneously\n",
    "\n",
    "2. **Model Dependencies**\n",
    "    - Prior Sensitivity: Results depend on prior specifications\n",
    "    - Model Misspecification: Wrong likelihood can lead to wrong conclusions\n",
    "    - Computational Limits: More change points increase computation time\n",
    "\n",
    "3. **Data Limitations**\n",
    "    - Frequency: Daily data may miss intraday changes\n",
    "    - Completeness: Not all influencing factors captured\n",
    "    - Quality: Historical data quality varies\n",
    "\n",
    "4. **Interpretation Challenges**\n",
    "    - Multiple Solutions: Different models may suggest different change points\n",
    "    - Uncertainty: Probabilistic outputs require careful interpretation\n",
    "    - Expert Judgment Needed: Statistical results need domain expertise context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cea057",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a3aaf",
   "metadata": {},
   "source": [
    "### Key Findings from Time Series Analysis:\n",
    "1. **Data Properties**:\n",
    "    - Non-stationary price levels, stationary returns\n",
    "    - Clear upward trend with cyclical patterns\n",
    "    - Time-varying volatility with clustering\n",
    "    - Non-normal distribution (fat tails)\n",
    "2. **Modeling Implications**:\n",
    "    - Need to handle non-stationarity (differencing or explicit trend modeling)\n",
    "    - Should account for changing volatility (stochastic volatility or GARCH)\n",
    "    - Change point models appropriate for detecting structural breaks\n",
    "    - Bayesian approach provides uncertainty quantification\n",
    "3. **Expected Value**:\n",
    "    - Identify significant market regime changes\n",
    "    - Quantify event impacts on price dynamics\n",
    "    - Provide actionable insights for stakeholders\n",
    "\n",
    "**Next steps**: Implement Bayesian change point model using PyMC.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
